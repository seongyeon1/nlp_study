{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b26ed917",
   "metadata": {},
   "source": [
    "## 1. Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b32f226",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38e78fcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/seongyeon-\n",
      "[nltk_data]     kim/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "972abfa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 토큰화 :  ['Do', \"n't\", 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', ',', 'Mr.', 'Jones', \"'s\", 'Orphange', 'is', 'a', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop', '.']\n"
     ]
    }
   ],
   "source": [
    "print('단어 토큰화 : ', word_tokenize(\"Don't be fooled by the dark sounding name, Mr. Jones's Orphange is a cheery as cheery goes for a pastry shop.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c41cfc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 토큰화2 :  ['Don', \"'\", 't', 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', ',', 'Mr', '.', 'Jones', \"'\", 's', 'Orphange', 'is', 'a', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop', '.']\n"
     ]
    }
   ],
   "source": [
    "print('단어 토큰화2 : ', WordPunctTokenizer().tokenize(\"Don't be fooled by the dark sounding name, Mr. Jones's Orphange is a cheery as cheery goes for a pastry shop.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23697342",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 토큰화3 : [\"don't\", 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', 'mr', \"jones's\", 'orphange', 'is', 'a', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop']\n"
     ]
    }
   ],
   "source": [
    "print('단어 토큰화3 :', text_to_word_sequence(\"Don't be fooled by the dark sounding name, Mr. Jones's Orphange is a cheery as cheery goes for a pastry shop.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27957e49",
   "metadata": {},
   "source": [
    "### 표준 토큰화 예제\n",
    "- 규칙 1. 하이푼으로 구성된 단어는 하나로 유지한다.\n",
    "- 규칙 2. doesn't와 같이 아포스트로피로 '접어'가 함께하는 단어는 분리해준다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4adf7234",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "트리뱅크 워드토크나이저 : ['Starting', 'a', 'home-based', 'restaurant', 'may', 'be', 'an', 'ideal.', 'it', 'does', \"n't\", 'have', 'a', 'food', 'chain', 'or', 'restaurant', 'of', 'their', 'own', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "\n",
    "text = \"Starting a home-based restaurant may be an ideal. it doesn't have a food chain or restaurant of their own.\"\n",
    "print('트리뱅크 워드토크나이저 :', tokenizer.tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa193cd",
   "metadata": {},
   "source": [
    "### 문장 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8906469f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문장 토큰화1 : ['His barber kept his word.', 'But keeping such a huge secret to himself was driving him crazy.', 'Finally, the barber went up a mountain and almost to the edge of a cliff.', 'He dug a hole in the midst of some reeds.', 'He looked about, to make sure no one was near.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "text = \"His barber kept his word. But keeping such a huge secret to himself was driving him crazy. Finally, the barber went up a mountain and almost to the edge of a cliff. He dug a hole in the midst of some reeds. He looked about, to make sure no one was near.\"\n",
    "print('문장 토큰화1 :', sent_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b0ba12c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문장 토큰화2 : ['I am actively looking for Ph.D. students.', 'and you are a Ph.D student.']\n"
     ]
    }
   ],
   "source": [
    "text = \"I am actively looking for Ph.D. students. and you are a Ph.D student.\"\n",
    "print('문장 토큰화2 :',sent_tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d410db2",
   "metadata": {},
   "source": [
    "- 한국어 문장 토큰화 도구"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "83c851b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting kss\n",
      "  Downloading kss-3.4.3.tar.gz (42.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.4/42.4 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting emoji==1.2.0\n",
      "  Downloading emoji-1.2.0-py3-none-any.whl (131 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.3/131.3 kB\u001b[0m \u001b[31m992.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: regex in /Users/seongyeon-kim/miniforge3/lib/python3.9/site-packages (from kss) (2022.8.17)\n",
      "Collecting more_itertools\n",
      "  Downloading more_itertools-8.14.0-py3-none-any.whl (52 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.2/52.2 kB\u001b[0m \u001b[31m907.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: kss\n",
      "  Building wheel for kss (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for kss: filename=kss-3.4.3-py3-none-any.whl size=42448067 sha256=cb86e43a0c950aef956e7f40bf2815a90b8f833535cbb5116d172b79fd89a448\n",
      "  Stored in directory: /Users/seongyeon-kim/Library/Caches/pip/wheels/f5/41/62/17164eacd29bf80f19548f0439b94c75620828409ee3274e01\n",
      "Successfully built kss\n",
      "Installing collected packages: emoji, more_itertools, kss\n",
      "Successfully installed emoji-1.2.0 kss-3.4.3 more_itertools-8.14.0\n"
     ]
    }
   ],
   "source": [
    "!pip install kss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4ce227d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Korean Sentence Splitter]: Initializing Pynori...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "한국어 문장 토큰화 :  ['딥 러닝 자연어 처리가 재미있기는 합니다.', '그런데 문제는 영어보다 한국어로 할 때 너무 어렵습니다.', '이제 해보면 알걸요?']\n"
     ]
    }
   ],
   "source": [
    "import kss\n",
    "\n",
    "text = '딥 러닝 자연어 처리가 재미있기는 합니다. 그런데 문제는 영어보다 한국어로 할 때 너무 어렵습니다. 이제 해보면 알걸요?'\n",
    "print('한국어 문장 토큰화 : ', kss.split_sentences(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b866700",
   "metadata": {},
   "source": [
    "### 품사 태깅(Part-of-speech tagging)\n",
    "- NLTK와 KoNLPy를 이용한 영어, 한국어 토큰화 실습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c06f8be1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 토큰화 : ['I', 'am', 'actively', 'looking', 'for', 'Ph.D.', 'students', '.', 'and', 'you', 'are', 'a', 'Ph.D', 'student', '.']\n",
      "품사 태깅 : [('I', 'PRP'), ('am', 'VBP'), ('actively', 'RB'), ('looking', 'VBG'), ('for', 'IN'), ('Ph.D.', 'NNP'), ('students', 'NNS'), ('.', '.'), ('and', 'CC'), ('you', 'PRP'), ('are', 'VBP'), ('a', 'DT'), ('Ph.D', 'NNP'), ('student', 'NN'), ('.', '.')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/seongyeon-kim/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "text = \"I am actively looking for Ph.D. students. and you are a Ph.D student.\"\n",
    "tokenized_sentence = word_tokenize(text)\n",
    "\n",
    "print('단어 토큰화 :', tokenized_sentence)\n",
    "print('품사 태깅 :', pos_tag(tokenized_sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab576c0d",
   "metadata": {},
   "source": [
    "- 영어 문장에 대해서 토큰화를 수행한 결과를 입력으로 품사 태깅을 수행하였습니다. Penn Treebank POG Tags에서 PRP는 인칭 대명사, VBP는 동사, RB는 부사, VBG는 현재부사, IN은 전치사, NNP는 고유 명사, NNS는 복수형 명사, CC는 접속사, DT는 관사를 의미합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fdc2f94",
   "metadata": {},
   "source": [
    "- 한국어 자연어 처리를 위해서는 KoNLPy라는 파이썬 패키지를 사용할 수 있습니다. 코엔엘파이를 통해서 사용할 수 있는 형태소 분석기로 Okt(Open Korea Text), 메캅(Mecab), 코모란(Komoran), 한나눔(Hannanum), 꼬꼬마(Kkma)가 있습니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a4d838f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.020s][warning][os,thread] Attempt to protect stack guard pages failed (0x0000000169c98000-0x0000000169ca4000).\n",
      "[0.020s][warning][os,thread] Attempt to deallocate stack guard pages failed.\n",
      "OKT 형태소 분석 : ['열심히', '코딩', '한', '당신', ',', '연휴', '에는', '여행', '을', '가봐요']\n",
      "OKT 품사 태깅 : [('열심히', 'Adverb'), ('코딩', 'Noun'), ('한', 'Josa'), ('당신', 'Noun'), (',', 'Punctuation'), ('연휴', 'Noun'), ('에는', 'Josa'), ('여행', 'Noun'), ('을', 'Josa'), ('가봐요', 'Verb')]\n",
      "OKT 명사 추출 : ['코딩', '당신', '연휴', '여행']\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Okt\n",
    "from konlpy.tag import Kkma\n",
    "\n",
    "okt = Okt()\n",
    "kkma = Kkma()\n",
    "\n",
    "print('OKT 형태소 분석 :', okt.morphs('열심히 코딩한 당신, 연휴에는 여행을 가봐요'))\n",
    "print('OKT 품사 태깅 :', okt.pos('열심히 코딩한 당신, 연휴에는 여행을 가봐요'))\n",
    "print('OKT 명사 추출 :', okt.nouns('열심히 코딩한 당신, 연휴에는 여행을 가봐요'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "735a44fe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "꼬꼬마 형태소 분석 : ['열심히', '코딩', '하', 'ㄴ', '당신', ',', '연휴', '에', '는', '여행', '을', '가보', '아요']\n",
      "꼬꼬마 품사 태깅 : [('열심히', 'MAG'), ('코딩', 'NNG'), ('하', 'XSV'), ('ㄴ', 'ETD'), ('당신', 'NP'), (',', 'SP'), ('연휴', 'NNG'), ('에', 'JKM'), ('는', 'JX'), ('여행', 'NNG'), ('을', 'JKO'), ('가보', 'VV'), ('아요', 'EFN')]\n",
      "꼬꼬마 명사 추출 : ['코딩', '당신', '연휴', '여행']\n"
     ]
    }
   ],
   "source": [
    "print('꼬꼬마 형태소 분석 :',kkma.morphs(\"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"))\n",
    "print('꼬꼬마 품사 태깅 :',kkma.pos(\"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"))\n",
    "print('꼬꼬마 명사 추출 :',kkma.nouns(\"열심히 코딩한 당신, 연휴에는 여행을 가봐요\")) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4d9924",
   "metadata": {},
   "source": [
    "## 2. Cleaning & Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25b549d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " was wondering anyone out there could enlighten this car.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "text = 'I was wondering if anyone out there could enlighten me on this car.'\n",
    "\n",
    "# 길이가 1~2인 단어들을 정규 표현식을 이용하여 삭제\n",
    "shortword = re.compile(r'\\W*\\b\\w{1,2}\\b')\n",
    "print(shortword.sub('',text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b42412",
   "metadata": {},
   "source": [
    "## 3. 어간 추출(Stemming) and 표제어 추출(Lemmatization)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b970997",
   "metadata": {},
   "source": [
    "### 3.1 Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "081a43f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/seongyeon-\n",
      "[nltk_data]     kim/nltk_data...\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0472a479",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "48aabed2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to /Users/seongyeon-\n",
      "[nltk_data]     kim/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c5bc320f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "표제어 추출 전 : ['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\n",
      "표제어 추출 후 : ['policy', 'doing', 'organization', 'have', 'going', 'love', 'life', 'fly', 'dy', 'watched', 'ha', 'starting']\n"
     ]
    }
   ],
   "source": [
    "print('표제어 추출 전 :', words)\n",
    "print('표제어 추출 후 :', [lemmatizer.lemmatize(word) for word in words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "809c89ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'die'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('dies', 'v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3a2b62e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'watch'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('watched', 'v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "18990198",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'have'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('has', 'v')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7301e852",
   "metadata": {},
   "source": [
    "### 3.2 Stemming\n",
    "- 어간 추출은 형태학적 분석을 단순화한 버전이라고 볼 수도 있고, 정해진 규칙만 보고 단어의 어미를 자르는 어림짐작의 작업이라고 볼 수도 있습니다. 이 작업은 섬세한 작업이 아니기 때문에 어간 추출후에 나오는 결과 단어는 사전에 존재하지 않는 단어일 수도 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c156092a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "어간 추출 전 : ['This', 'was', 'not', 'the', 'map', 'we', 'found', 'in', 'Billy', 'Bones', \"'s\", 'chest', ',', 'but', 'an', 'accurate', 'copy', ',', 'complete', 'in', 'all', 'things', '--', 'names', 'and', 'heights', 'and', 'soundings', '--', 'with', 'the', 'single', 'exception', 'of', 'the', 'red', 'crosses', 'and', 'the', 'written', 'notes']\n",
      "어간 추출 후 : ['thi', 'wa', 'not', 'the', 'map', 'we', 'found', 'in', 'billi', 'bone', \"'s\", 'chest', ',', 'but', 'an', 'accur', 'copi', ',', 'complet', 'in', 'all', 'thing', '--', 'name', 'and', 'height', 'and', 'sound', '--', 'with', 'the', 'singl', 'except', 'of', 'the', 'red', 'cross', 'and', 'the', 'written', 'note']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "sentence = \"This was not the map we found in Billy Bones's chest, but an accurate copy, complete in all things--names and heights and soundings--with the single exception of the red crosses and the written notes\"\n",
    "tokenized_sentence = word_tokenize(sentence)\n",
    "\n",
    "print('어간 추출 전 :', tokenized_sentence)\n",
    "print('어간 추출 후 :', [stemmer.stem(word) for word in tokenized_sentence])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056ea964",
   "metadata": {},
   "source": [
    "### 포터 알고리즘의 어간 추출 규칙\n",
    "- ALIZE -> AL\n",
    "- ANCE -> 제거\n",
    "- ICAL -> IC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "428e1432",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "어간 추출 전 : ['formalize', 'allowance', 'electricical']\n",
      "어간 추출 후 : ['formal', 'allow', 'electric']\n"
     ]
    }
   ],
   "source": [
    "words = ['formalize', 'allowance', 'electricical']\n",
    "\n",
    "print('어간 추출 전 :', words)\n",
    "print('어간 추출 후 :', [stemmer.stem(word) for word in words])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6af95a6",
   "metadata": {},
   "source": [
    "- 어간 추출 속도는 표제어 추출보다 일반적으로 빠른데, 포터 어간 추출기는 정밀하게 설계되어 정확도가 높으므로 영어 자연어 처리에서 어간 추출을 하고자 한다면 가장 준수한 선택입니다. NLTK에서는 포터 알고리즘 외에도 랭커스터 스태머(Lancaster Stemmer) 알고리즘을 지원합니다. 이번에는 포터 알고리즘과 랭커스터 스태머 알고리즘으로 각각 어간 추출을 진행했을 때, 이 둘의 결과를 비교해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b0933203",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "어간 추출 전 : ['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\n",
      "포터 스테머의 어간 추출 후 : ['polici', 'do', 'organ', 'have', 'go', 'love', 'live', 'fli', 'die', 'watch', 'ha', 'start']\n",
      "랭커스터 스테머의 어간 추출 후 : ['policy', 'doing', 'org', 'hav', 'going', 'lov', 'liv', 'fly', 'die', 'watch', 'has', 'start']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "\n",
    "porter_stemmer = PorterStemmer()\n",
    "lancaster_stemmer = LancasterStemmer()\n",
    "\n",
    "words = ['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\n",
    "print('어간 추출 전 :', words)\n",
    "print('포터 스테머의 어간 추출 후 :', [porter_stemmer.stem(word) for word in words])\n",
    "print('랭커스터 스테머의 어간 추출 후 :', [lancaster_stemmer.stem(word) for word in words])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16fbe078",
   "metadata": {},
   "source": [
    "## 4. Stopword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "809807c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from konlpy.tag import Okt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89045f7",
   "metadata": {},
   "source": [
    "### 4.1 NLTK에서 불용어 확인하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fd155a22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/seongyeon-\n",
      "[nltk_data]     kim/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "07a3b15e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "불용어 개수 : 179\n",
      "불용어 10개 출력 : ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]\n"
     ]
    }
   ],
   "source": [
    "stop_words_list = stopwords.words('english')\n",
    "print('불용어 개수 :', len(stop_words_list))\n",
    "print('불용어 10개 출력 :', stop_words_list[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89fa920",
   "metadata": {},
   "source": [
    "### 4.2 NLTK를 통해서 불용어 제거하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6edf9720",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = \"Family is not an important thing. It's everything.\"\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8e6a713a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "불용어 제거 전 : ['Family', 'is', 'not', 'an', 'important', 'thing', '.', 'It', \"'s\", 'everything', '.']\n",
      "불용어 제거 후 : ['Family', 'important', 'thing', '.', 'It', \"'s\", 'everything', '.']\n"
     ]
    }
   ],
   "source": [
    "word_tokens = word_tokenize(example)\n",
    "\n",
    "result = []\n",
    "for word in word_tokens:\n",
    "    if word not in stop_words:\n",
    "        result.append(word)\n",
    "        \n",
    "print('불용어 제거 전 :', word_tokens)\n",
    "print('불용어 제거 후 :', result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de4bdef",
   "metadata": {},
   "source": [
    "### 4.3 한국어에서 불용어 제거하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "23a00b9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "불용어 제거 전 : ['고기', '를', '아무렇게나', '구', '우려', '고', '하면', '안', '돼', '.', '고기', '라고', '다', '같은', '게', '아니거든', '.', '예컨대', '삼겹살', '을', '구울', '때', '는', '중요한', '게', '있지', '.']\n",
      "불용어 제거 후 : ['고기', '하면', '.', '고기', '라고', '다', '아니거든', '.', '예컨대', '삼겹살', '을', '중요한', '있지', '.']\n"
     ]
    }
   ],
   "source": [
    "okt = Okt()\n",
    "\n",
    "example = \"고기를 아무렇게나 구우려고 하면 안 돼. 고기라고 다 같은 게 아니거든. 예컨대 삼겹살을 구울 때는 중요한 게 있지.\"\n",
    "stop_words = \"를 아무렇게나 구 우려 고 안 돼 같은 게 구울 때 는\"\n",
    "\n",
    "stop_words = set(stop_words.split(' '))\n",
    "word_tokens = okt.morphs(example)\n",
    "\n",
    "result = [word for word in word_tokens if not word in stop_words]\n",
    "\n",
    "print('불용어 제거 전 :',word_tokens) \n",
    "print('불용어 제거 후 :',result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c012999",
   "metadata": {},
   "source": [
    "- 추가 참고 가능한 한국어 불용어 리스트 : https://bab2min.tistory.com/544"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1448c4b3",
   "metadata": {},
   "source": [
    "## 5. 정규 표현식(Regular Expression)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b9c252",
   "metadata": {},
   "source": [
    "### 5.1 정규 표현식 문법과 모듈 함수"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b203bd4c",
   "metadata": {},
   "source": [
    "- 정규 표현식 문법\n",
    "|특수문자|설명|\n",
    "|---|---|\n",
    "|.|한 개의 임의의 문자를 나타냅니다. (줄바꿈 문자인 \\n는 제외)|\n",
    "|?|앞의 문자가 존재할 수도 있고, 존재하지 않을 수도 있습니다. (문자가 0개 또는 1개)|\n",
    "|+|앞의 문자가 최소 한 개 이상 존재합니다. (문자가 1개 이상)\n",
    "|^|뒤의 문자열로 문자열이 시작됩니다.|\n",
    "|$|앞의 문자열로 문자열이 끝납니다.|\n",
    "|{숫자}|숫자만큼 반복합니다.|\n",
    "|{숫자1, 숫자2}|숫자1 이상 숫자2 이하만큼 반복합니다. ?, *, +를 이것으로 대체할 수 있습니다.|\n",
    "|{숫자,}|숫자 이상만큼 반복합니다.|\n",
    "|[ ]|대괄호 안의 문자들 중 한 개의 문자와 매치합니다. [amk]라고 한다면 a 또는 m 또는 k 중 하나라도 존재하면 매치를 의미합니다. [a-z]와 같이 범위를 지정할 수도 있습니다. [a-zA-Z]는 알파벳 전체를 의미하는 범위이며, 문자열에 알파벳이 존재하면 매치를 의미합니다.|\n",
    "|[^문자]|해당 문자를 제외한 문자를 매치합니다.|\n",
    "| \\| | A\\|B와 같이 쓰이며 A 또는 B의 의미를 가집니다.|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120765bf",
   "metadata": {},
   "source": [
    "- 정규표현식 모듈 함수\n",
    "|모듈 함수|설명|\n",
    "|---|---|\n",
    "|re.compile()|정규표현식을 컴파일하는 함수입니다. 다시 말해, 파이썬에게 전해주는 역할을 합니다. 찾고자 하는 패턴이 빈번한 경우에는 미리 컴파일 해놓고 사용하면 속도와 편의성면에서 유리합니다.|\n",
    "|re.search()|문자열 전체에 대해서 정규표현식과 매치되는지를 검색합니다.|\n",
    "|re.match()|문자열의 처음이 정규표현식과 매치되는지를 검색합니다.|\n",
    "|re.split()|정규 표현식을 기준으로 문자열을 분리하여 리스트로 리턴합니다.|\n",
    "|re.findall()|문자열에서 정규 표현식과 매치되는 모든 경우의 문자열을 찾아서 리스트로 리턴합니다. 만약, 매치되는 문자열이 없다면 빈 리스트가 리턴됩니다.|\n",
    "|re.finditer()|문자열에서 정규 표현식과 매치되는 모든 문자열에 대한 이터레이터 객체를 리턴합니다.|\n",
    "|re.sub()|문자열에서 정규 표현식과 일치하는 부분에 대해서 다른 문자열로 대체합니다.|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684e3d99",
   "metadata": {},
   "source": [
    "### 5.2 정규 표현식 실습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "345af038",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7392a149",
   "metadata": {},
   "source": [
    "- .은 한 개의 임의의 문자를 나타냅니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fda9495c",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = re.compile('a.c')\n",
    "r.search('kkk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "14af4ac9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 3), match='abc'>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.search('abc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48120a80",
   "metadata": {},
   "source": [
    "- ?는 ?앞의 문자가 존재할 수도 있고 존재하지 않을 수도 있는 경우를 나타냅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "75020e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = re.compile('ab?c')\n",
    "r.search('abbc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4ffa218d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 3), match='abc'>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.search('abc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068a6be6",
   "metadata": {},
   "source": [
    "- * 은 바로 앞의 문자가 0개 이상일 경우를 나타냅니다. 앞의 문자는 존재하지 않을 수도 있으며, 여러 개일 수도 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "878193f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = re.compile('ab*c')\n",
    "r.search('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a150385c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 2), match='ac'>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.search('ac')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4231066e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 3), match='abc'>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.search('abc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "98841168",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 7), match='abbbbbc'>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.search('abbbbbc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a5a96e",
   "metadata": {},
   "source": [
    "- +는 *와 유사합니다. 다른 점은 앞의 문자가 최소 1개 이상이어야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a44ca0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = re.compile('ab+c')\n",
    "r.search('ac')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "95fb6fe1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 3), match='abc'>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.search('abc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0d8b09b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 6), match='abbbbc'>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.search('abbbbc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71106d9e",
   "metadata": {},
   "source": [
    "- ^는 시작되는 문자열을 지정합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6c6e5a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = re.compile('^ab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "23adc46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "r.search('bbc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f20c056e",
   "metadata": {},
   "outputs": [],
   "source": [
    "r.search('zab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3d3cc42a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 2), match='ab'>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.search('abz')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479170ce",
   "metadata": {},
   "source": [
    "- {숫자} 기호 : 문자에 해당 기호를 붙이면, 해당 문자를 숫자만큼 반복한 것을 나타냅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a3e45d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = re.compile('ab{2}c')\n",
    "\n",
    "r.search('ac')\n",
    "r.search('abc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8d14c244",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 4), match='abbc'>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.search('abbc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78597f87",
   "metadata": {},
   "source": [
    "- {숫자1, 숫자2} 기호 : 문자에 해당 기호를 붙이면, 해당 문자를 숫자1 이상 숫자2 이하만큼 반복합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "83080cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = re.compile('ab{2,8}c')\n",
    "\n",
    "r.search('ac')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7717e40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "r.search('abc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c9bbf539",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 8), match='abbbbbbc'>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.search('abbbbbbc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b95a582a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 4), match='abbc'>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.search('abbc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "15779c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "r.search('abbbbbbbbbbbbbc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a95f1bdf",
   "metadata": {},
   "source": [
    "- {숫자,} 기호 : 해당 문자를 숫자 이상만큼 반복합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58aab86",
   "metadata": {},
   "source": [
    "- \\[ \\] 안에 문자들을 넣으면 그 문자들 중 한 개의 문자와 매치라는 의미를 가집니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "bc2be6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = re.compile('[abc]')\n",
    "r.search('zzz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "1fea34df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 1), match='a'>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.search('a')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff3c83d",
   "metadata": {},
   "source": [
    "- [^문자]는 ^기호 뒤에 붙은 문자들을 제외한 모든 문자를 매치하는 역할을 합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "466b49a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = re.compile('[^abc]')\n",
    "\n",
    "r.search('a')\n",
    "r.search('ab')\n",
    "r.search('b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a9121089",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 1), match='d'>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.search('d')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da552660",
   "metadata": {},
   "source": [
    "## 5.3 정규 표현식 모듈 함수 예제\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6501e9",
   "metadata": {},
   "source": [
    "### 1. re.match()와 re.search()의 차이\n",
    "- search()가 정규 표현식 전체에 대해서 문자열이 매치하는지를 본다면, match()는 문자열의 첫 부분부터 정규표현식과 매치하는지를 확인합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "77135f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = re.compile('ab.')\n",
    "r.match('kkkabc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "ca322356",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(3, 6), match='abc'>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.search('kkkabc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8710037",
   "metadata": {},
   "source": [
    "### 2. re.split()\n",
    "- split() 함수는 입력된 정규 표현식을 기준으로 문자열들을 분리하여 리스트로 리턴합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "be187db9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['사과', '딸기', '수박', '메론', '바나나']"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = '사과 딸기 수박 메론 바나나'\n",
    "re.split(' ', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "ddbe4a64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['사과', '딸기', '수박', '메론', '바나나']"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = '''사과\n",
    "딸기\n",
    "수박\n",
    "메론\n",
    "바나나'''\n",
    "\n",
    "re.split('\\n', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12346c84",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
